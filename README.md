This project shows different explainable AI methods (attention-based explanations, LIME, SHAP, saliency maps, Grad-CAM and LayerCAM) for detecting bias in tiger recognition systems.

Usage:

1. Clone the repository: git clone https://github.com/Michel0301/Explainable_AI_assignment_s1090290.git
2. Install dependencies: pip install -r requirements.txt
3. Open the Jupyter Notebook “finalAssignment_wildlife_notebook.ipynb” using Jupyter Notebook or Google Colab. 
4. Find the images.zip file that you cloned from my repository and upload this to the Google Colab (or other IDE) directory environment
5. Run the Jupyter Notebook “finalAssignment_wildlife_notebook.ipynb” using Jupyter Notebook or Google Colab.
